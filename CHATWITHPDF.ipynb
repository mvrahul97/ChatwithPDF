{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8kyaslF9vGdtI659aAz/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvrahul97/ChatwithPDF/blob/main/CHATWITHPDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye9JBdbhyp91"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import openai, os, requests, tempfile\n",
        "from dotenv import load_dotenv\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "from sqlalchemy.exc import ProgrammingError\n",
        "from pdfminer.high_level import extract_text as pdf_extract_text\n",
        "from streamlit_lottie import st_lottie_spinner\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Function to load Lottie animations using URL\n",
        "@st.cache_data\n",
        "def load_lottieurl(url):\n",
        "    \"\"\"\n",
        "    Fetches and caches a Lottie animation from a provided URL.\n",
        "\n",
        "    Args:\n",
        "    url (str): The URL of the Lottie animation.\n",
        "\n",
        "    Returns:\n",
        "    dict: The Lottie animation JSON or None if the request fails.\n",
        "    \"\"\"\n",
        "    r = requests.get(url)  # Perform the GET request\n",
        "    if r.status_code != 200:\n",
        "        return None  # Return None if request failed\n",
        "    return r.json()  # Return the JSON content of the Lottie animation\n",
        "\n",
        "# Load a specific Lottie animation to be used in the app\n",
        "loading_animation = load_lottieurl('https://lottie.host/5ac92c74-1a02-40ff-ac96-947c14236db1/u4nCMW6fXU.json')\n",
        "\n",
        "# Class for processing uploaded PDFs before user interaction\n",
        "class PreRunProcessor:\n",
        "    \"\"\"\n",
        "    Processes uploaded PDF files by extracting text and generating embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the processor with an OpenAI API key and a connection to a PostgreSQL database.\n",
        "        \"\"\"\n",
        "        # Load OpenAI API key from environment variables\n",
        "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        # Establish connection to the PostgreSQL database from the Supabase platform\n",
        "        self.engine = create_engine(os.getenv(\"SUPABASE_POSTGRES_URL\"), echo=True, client_encoding='utf8')\n",
        "        # Create a session maker bound to this engine\n",
        "        self.Session = sessionmaker(bind=self.engine)\n",
        "\n",
        "    def pdf_to_text(self, uploaded_file, chunk_length: int = 1000) -> list:\n",
        "        \"\"\"\n",
        "        Extracts text from the uploaded PDF and splits it into manageable chunks.\n",
        "\n",
        "        Args:\n",
        "        uploaded_file (UploadedFile): The PDF file uploaded by the user.\n",
        "        chunk_length (int): The desired length of each text chunk.\n",
        "\n",
        "        Returns:\n",
        "        list: A list of text chunks ready for embedding generation.\n",
        "        \"\"\"\n",
        "        # Extract text from the uploaded PDF\n",
        "        text = pdf_extract_text(uploaded_file)\n",
        "        # Split the text into chunks\n",
        "        chunks = [text[i:i + chunk_length].replace('\\n', '') for i in range(0, len(text), chunk_length)]\n",
        "        return self._generate_embeddings(chunks)\n",
        "\n",
        "    def define_vector_store(self, embeddings: list) -> bool:\n",
        "        \"\"\"\n",
        "        Stores the generated embeddings in the database.\n",
        "\n",
        "        Args:\n",
        "        embeddings (list): A list of dictionaries containing text and their corresponding embeddings.\n",
        "\n",
        "        Returns:\n",
        "        bool: True if the operation succeeds, False otherwise.\n",
        "        \"\"\"\n",
        "        session = self.Session()  # Create a new database session\n",
        "        try:\n",
        "            # Truncate the existing table and insert new embeddings\n",
        "            session.execute(text(\"TRUNCATE TABLE pdf_holder RESTART IDENTITY CASCADE;\"))\n",
        "            for embedding in embeddings:\n",
        "                # Insert each embedding into the pdf_holder table\n",
        "                session.execute(text(\"INSERT INTO pdf_holder (text, embedding) VALUES (:text, :embedding)\"), {\"text\": embedding[\"text\"], \"embedding\": embedding[\"vector\"]})\n",
        "            session.commit()  # Commit the changes\n",
        "            return True\n",
        "        except ProgrammingError as e:\n",
        "            if 'relation \"pdf_holder\" does not exist' in str(e.orig.pgerror):\n",
        "                # If the table doesn't exist, create it and the necessary extension\n",
        "                session.rollback()\n",
        "                session.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector;\"))\n",
        "                session.execute(text(\"\"\"\n",
        "                    CREATE TABLE pdf_holder (\n",
        "                        id SERIAL PRIMARY KEY,\n",
        "                        text TEXT,\n",
        "                        embedding VECTOR(3072)\n",
        "                    );\n",
        "                \"\"\"))\n",
        "                session.commit()\n",
        "                return False\n",
        "            else:\n",
        "                raise\n",
        "        finally:\n",
        "            session.close()  # Close the session\n",
        "\n",
        "    def _generate_embeddings(self, chunks: list) -> list:\n",
        "        \"\"\"\n",
        "        Generates embeddings for each text chunk using the OpenAI API.\n",
        "\n",
        "        Args:\n",
        "        chunks (list): A list of text chunks.\n",
        "\n",
        "        Returns:\n",
        "        list: A list of dictionaries containing text chunks and their corresponding embeddings.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Generate embeddings for the text chunks\n",
        "            response = openai.embeddings.create(model='text-embedding-3-large', input=chunks)\n",
        "            return [{\"vector\": embedding_info.embedding, \"text\": chunks[embedding_info.index]} for embedding_info in response.data]\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred during embeddings generation: {e}\")\n",
        "            return []\n",
        "\n",
        "# Function to process the uploaded PDF before any user interaction\n",
        "def process_pre_run(uploaded_file):\n",
        "    \"\"\"\n",
        "    Orchestrates the preprocessing of the uploaded PDF file, including text extraction and embedding generation.\n",
        "\n",
        "    Args:\n",
        "    uploaded_file (UploadedFile): The PDF file uploaded by the user.\n",
        "    \"\"\"\n",
        "    processor_class = PreRunProcessor()  # Instantiate the PreRunProcessor class\n",
        "    embeddings = processor_class.pdf_to_text(uploaded_file)  # Extract text and generate embeddings\n",
        "    if not embeddings or not processor_class.define_vector_store(embeddings):\n",
        "        st.error(\"Failed to store the PDF embedding.\")  # Notify if embedding storage fails\n",
        "    else:\n",
        "        st.success(\"PDF successfully uploaded. We can proceed now...\")  # Notify on successful processing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### Intent services #####\n",
        "\n",
        "class IntentService:\n",
        "    \"\"\"\n",
        "    Handles the detection of malicious intent in user queries, conversion of questions to embeddings,\n",
        "    and checks the relatedness of questions to PDF content via database queries.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the IntentService with the OpenAI API key and a connection to a PostgreSQL database.\n",
        "        \"\"\"\n",
        "        # Retrieve OpenAI API key from environment variables\n",
        "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        # Establish a connection to the PostgreSQL database hosted on the Supabase platform\n",
        "        self.engine = create_engine(os.getenv(\"SUPABASE_POSTGRES_URL\"), echo=True, client_encoding='utf8')\n",
        "\n",
        "    def detect_malicious_intent(self, question):\n",
        "        \"\"\"\n",
        "        Uses OpenAI's moderation model to detect malicious intent in a user's question.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's question as a string.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A boolean indicating if the question was flagged and a message explaining the result.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Create a moderation request to OpenAI API with the provided question\n",
        "            response = openai.moderations.create(model=\"text-moderation-latest\", input=question)\n",
        "            # Determine if the question was flagged as malicious\n",
        "            is_flagged = response.results[0].flagged\n",
        "            if is_flagged:\n",
        "                # Return true and a message if flagged\n",
        "                return is_flagged, \"This question has been flagged for malicious or inappropriate content...\"\n",
        "            else:\n",
        "                # Return false and a message if not flagged\n",
        "                return is_flagged, \"No malicious intent detected...\"\n",
        "        except Exception as e:\n",
        "            # Return none and an error message in case of an exception\n",
        "            return None, f\"Error in moderation: {str(e).split('. ')[0]}.\"\n",
        "\n",
        "    def query_database(self, query):\n",
        "        \"\"\"\n",
        "        Executes a SQL query on the connected PostgreSQL database and returns the first result.\n",
        "\n",
        "        Args:\n",
        "            query (str): SQL query string to be executed.\n",
        "\n",
        "        Returns:\n",
        "            sqlalchemy.engine.row.RowProxy or None: The first result row of the query or None if no results.\n",
        "        \"\"\"\n",
        "        # Connect to the database and execute the given query\n",
        "        with self.engine.connect() as connection:\n",
        "            result = connection.execute(text(query)).fetchone()\n",
        "            # Return the result if available; otherwise, return None\n",
        "            return result if result else None\n",
        "\n",
        "    def question_to_embeddings(self, question):\n",
        "        \"\"\"\n",
        "        Converts a user's question into vector embeddings using OpenAI's API.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's question as a string.\n",
        "\n",
        "        Returns:\n",
        "            list: The vectorized form of the question as a list or an empty list on failure.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Generate embeddings for the question using OpenAI's API\n",
        "            response = openai.embeddings.create(input=question, model=\"text-embedding-3-large\")\n",
        "            embedded_query = response.data[0].embedding\n",
        "            # Verify the dimensionality of the embedding\n",
        "            if len(embedded_query) != 3072:\n",
        "                raise ValueError(\"The dimensionality of the question embedding does not match the expected 3072 dimensions.\")\n",
        "            else:\n",
        "                # Convert the embedding to a numpy array and return it as a list\n",
        "                return np.array(embedded_query, dtype=np.float64).tolist()\n",
        "        except Exception as e:\n",
        "            # Log and return an empty list in case of an error\n",
        "            print(f\"Error embedding the question: {e}\")\n",
        "            return []\n",
        "\n",
        "    def check_relatedness_to_pdf_content(self, question):\n",
        "        \"\"\"\n",
        "        Determines if a user's question is related to PDF content stored in the database by querying for similar embeddings.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's question as a string.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A boolean indicating relatedness and a message explaining the result.\n",
        "        \"\"\"\n",
        "        # Convert the question to vector embeddings\n",
        "        question_vectorized = self.question_to_embeddings(question)\n",
        "\n",
        "        try:\n",
        "            # Query the database for the closest embedding to the question's embedding\n",
        "            with self.engine.connect() as conn:\n",
        "                result = conn.execute(text(\"\"\"\n",
        "                    SELECT id, text, embedding <=> CAST(:question_vector AS VECTOR) AS distance\n",
        "                    FROM pdf_holder\n",
        "                    ORDER BY distance ASC\n",
        "                    LIMIT 1;\n",
        "                \"\"\"), {'question_vector': question_vectorized}).fetchone()\n",
        "\n",
        "                if result:\n",
        "                    # Determine if the closest embedding is below a certain threshold\n",
        "                    _, _, distance = result\n",
        "                    threshold = 0.65  # Define a threshold for relatedness\n",
        "                    if distance < threshold:\n",
        "                        # Return true and a message if the question is related to the PDF content\n",
        "                        return True, \"Question is related to the PDF content...\"\n",
        "                    else:\n",
        "                        # Return false and a message if the question is not sufficiently related\n",
        "                        return False, \"Question is not related to the PDF content...\"\n",
        "                else:\n",
        "                    # Return false and a message if no embedding was found in the database\n",
        "                    return False, \"No match found in the database.\"\n",
        "        except Exception as e:\n",
        "            # Log and return false in case of an error during the database query\n",
        "            print(f\"Error searching the database: {e}\")\n",
        "            return False, f\"Error searching the database: {e}\"\n",
        "\n",
        "\n",
        "##### Information retrieval service #####\n",
        "\n",
        "class InformationRetrievalService:\n",
        "    \"\"\"\n",
        "    Provides services for searching vectorized questions within a vector store in the database.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the InformationRetrievalService with OpenAI API key and database connection.\n",
        "        \"\"\"\n",
        "        # Retrieve OpenAI API key from environment variables\n",
        "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        # Establish connection to the PostgreSQL database on the Supabase platform\n",
        "        self.engine = create_engine(os.getenv(\"SUPABASE_POSTGRES_URL\"), echo=True, client_encoding='utf8')\n",
        "        # Create a session maker bound to this engine\n",
        "        self.Session = sessionmaker(bind=self.engine)\n",
        "\n",
        "    def search_in_vector_store(self, vectorized_question: str, k: int = 1) -> str:\n",
        "        \"\"\"\n",
        "        Searches for the closest matching text in the vector store to a given vectorized question.\n",
        "\n",
        "        Args:\n",
        "            vectorized_question (str): The question converted into a vector.\n",
        "            k (int): The number of top results to retrieve, defaults to 1.\n",
        "\n",
        "        Returns:\n",
        "            str: The text of the closest matching document or an error message if no match is found.\n",
        "        \"\"\"\n",
        "        # SQL query to find the closest match in the vector store\n",
        "        sql_query = text(\"\"\"\n",
        "            SELECT id, text, embedding <=> CAST(:query_vector AS VECTOR) AS distance\n",
        "            FROM pdf_holder\n",
        "            ORDER BY distance\n",
        "            LIMIT :k\n",
        "        \"\"\")\n",
        "        # Execute the query with provided vectorized question and k value\n",
        "        with self.engine.connect() as conn:\n",
        "            results = conn.execute(sql_query, {'query_vector': vectorized_question, 'k': k}).fetchall()\n",
        "            if results:\n",
        "                # Return the text of the closest match if results are found\n",
        "                return results[0].text\n",
        "            else:\n",
        "                # Display an error if no matching documents are found\n",
        "                st.error(\"No matching documents found.\")\n",
        "\n",
        "##### Response service #####\n",
        "class ResponseService:\n",
        "    \"\"\"\n",
        "    Generates responses to user questions by integrating with OpenAI's ChatCompletion.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the ResponseService with the OpenAI API key.\n",
        "        \"\"\"\n",
        "        # Load OpenAI API key from environment variables\n",
        "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    def generate_response(self, question, retrieved_info):\n",
        "        \"\"\"\n",
        "        Generates a response using OpenAI's ChatCompletion API based on the provided question and retrieved information.\n",
        "\n",
        "        Args:\n",
        "            question (str): The user's question.\n",
        "            retrieved_info (str): Information retrieved that is related to the question.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated response or an error message if no response is available.\n",
        "        \"\"\"\n",
        "        # Generate a response using the ChatCompletion API with the question and retrieved information\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4-turbo-preview\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": 'Based on the FACTS, give a concise and detailed answer to the QUESTION.'+\n",
        "                f'QUESTION: {question}. FACTS: {retrieved_info}'}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if response.choices and response.choices[0].message.content:\n",
        "            # Return the generated response if available\n",
        "            return response.choices[0].message.content\n",
        "        else:\n",
        "            # Display an error if no content is generated\n",
        "            st.error(\"No content available.\")\n",
        "\n",
        "###### Independant & dependant of the function's class ######\n",
        "\n",
        "# Securely uploads a file to Google Drive and ensures the temporary file is deleted after upload\n",
        "def upload_to_google_drive(uploaded_file):\n",
        "    \"\"\"\n",
        "    Uploads a file to Google Drive using service account credentials, makes it publicly viewable,\n",
        "    and returns a shareable link for the uploaded file.\n",
        "\n",
        "    Args:\n",
        "        uploaded_file: The file uploaded by the user through the Streamlit interface.\n",
        "\n",
        "    Returns:\n",
        "        str: The shareable link of the uploaded file.\n",
        "    \"\"\"\n",
        "    # Define the scope for Google Drive API access to allow file uploading and sharing.\n",
        "    scope = ['https://www.googleapis.com/auth/drive.file', 'https://www.googleapis.com/auth/drive']\n",
        "\n",
        "    # Load Google Drive API credentials from Streamlit secrets (TOML format).\n",
        "    # These credentials are stored securely and used to authenticate with Google Drive.\n",
        "    credentials_info = st.secrets[\"google_credentials\"]\n",
        "\n",
        "    # Convert credentials info to a dictionary suitable for the oauth2client library.\n",
        "    # This step formats the credentials in a way that GoogleAuth can use for authentication.\n",
        "    credentials_dict = {key: value for key, value in credentials_info.items()}\n",
        "\n",
        "    # Authenticate using the service account credentials to access Google Drive.\n",
        "    credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = credentials  # Set the authenticated credentials\n",
        "    drive = GoogleDrive(gauth)  # Create a GoogleDrive instance with authenticated GoogleAuth instance\n",
        "\n",
        "    # Use a temporary file to store the uploaded file's content.\n",
        "    # This approach avoids loading the entire file content into memory, which is efficient for large files.\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='-' + uploaded_file.name) as temp_file:\n",
        "        temp_file.write(uploaded_file.read())  # Write the uploaded file content to the temporary file\n",
        "        temp_file_path = temp_file.name  # Store the path of the temporary file for later use in uploading\n",
        "\n",
        "    try:\n",
        "        # Create a new file on Google Drive using the uploaded file's name.\n",
        "        file_drive = drive.CreateFile({'title': uploaded_file.name})\n",
        "        # Set the content of the Google Drive file to that of the temporary file.\n",
        "        file_drive.SetContentFile(temp_file_path)\n",
        "        file_drive.Upload()  # Upload the file to Google Drive\n",
        "\n",
        "        # Change the uploaded file's sharing settings to make it viewable by anyone with the link.\n",
        "        file_drive.InsertPermission({\n",
        "            'type': 'anyone',\n",
        "            'value': 'anyone',\n",
        "            'role': 'reader'\n",
        "        })\n",
        "\n",
        "        # Format the shareable link for preview.\n",
        "        shareable_link = f\"https://drive.google.com/file/d/{file_drive['id']}/preview\"\n",
        "\n",
        "        # Print the shareable link for testing purposes.\n",
        "        st.write(\"Shareable link:\", shareable_link)\n",
        "\n",
        "    finally:\n",
        "        # Ensure the temporary file is deleted after the upload process is complete.\n",
        "        # This cleanup step prevents accumulation of temporary files on the server.\n",
        "        os.unlink(temp_file_path)\n",
        "\n",
        "\n",
        "# Orchestrates the processing of user questions regarding PDF content\n",
        "def intent_orchestrator(service_class, user_question):\n",
        "    \"\"\"\n",
        "    Orchestrates the process of checking a user's question for malicious intent and relevance to PDF content.\n",
        "\n",
        "    Args:\n",
        "        service_class: The class instance providing the services for intent detection and content relevance.\n",
        "        user_question: The question posed by the user.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the vectorized question and the original question if relevant, or (None, None) otherwise.\n",
        "    \"\"\"\n",
        "    # Detect malicious intent in the user's question\n",
        "    is_flagged, flag_message = service_class.detect_malicious_intent(user_question)\n",
        "    st.write(flag_message)  # Display the flag message\n",
        "\n",
        "    if is_flagged:\n",
        "        # If the question is flagged, do not process further\n",
        "        st.error(\"Your question was not processed. Please try a different question.\")\n",
        "        return (None, None)\n",
        "\n",
        "    # Check if the question is related to the PDF content\n",
        "    related, relatedness_message = service_class.check_relatedness_to_pdf_content(user_question)\n",
        "    st.write(relatedness_message)  # Display the relatedness message\n",
        "\n",
        "    if related:\n",
        "        # If the question is related, proceed with processing\n",
        "        vectorized_question = service_class.question_to_embeddings(user_question)\n",
        "        st.success(\"Your question was processed successfully. Now fetching an answer...\")\n",
        "        return (vectorized_question, user_question)\n",
        "    else:\n",
        "        # If not related, do not process further\n",
        "        st.error(\"Your question was not processed. Please try a different question.\")\n",
        "        return (None, None)\n",
        "\n",
        "# Starts the question processing workflow\n",
        "def process_user_question(service_class, user_question):\n",
        "    \"\"\"\n",
        "    Initiates the processing of a user's question through various services.\n",
        "\n",
        "    Args:\n",
        "        service_class: The class instance providing services for processing the user's question.\n",
        "        user_question: The question posed by the user.\n",
        "\n",
        "    Returns:\n",
        "        The result of the intent orchestration process.\n",
        "    \"\"\"\n",
        "    # Orchestrates the intent processing of the user's question\n",
        "    result = intent_orchestrator(service_class, user_question)\n",
        "    return result\n",
        "\n",
        "# Initiates the retrieval process for information related to the user's question\n",
        "def process_retrieval(vectorized_question: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Retrieves information related to the vectorized question from the vector store.\n",
        "\n",
        "    Args:\n",
        "        vectorized_question (str): The vectorized form of the user's question.\n",
        "\n",
        "    Returns:\n",
        "        Retrieved information related to the user's question.\n",
        "    \"\"\"\n",
        "    service = InformationRetrievalService()\n",
        "    retrieved_info = service.search_in_vector_store(vectorized_question)\n",
        "    return retrieved_info\n",
        "\n",
        "# Generates a response based on the user's question and the retrieved information\n",
        "def process_response(retrieved_info, question):\n",
        "    \"\"\"\n",
        "    Generates a response to the user's question based on retrieved information.\n",
        "\n",
        "    Args:\n",
        "        retrieved_info: Information related to the user's question retrieved from the vector store.\n",
        "        question: The original question posed by the user.\n",
        "\n",
        "    Returns:\n",
        "        A generated response to the user's question.\n",
        "    \"\"\"\n",
        "    response_service_processor = ResponseService()\n",
        "    final_response = response_service_processor.generate_response(question, retrieved_info)\n",
        "    return final_response\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    The main function to run the Streamlit app, including a PDF viewer.\n",
        "    \"\"\"\n",
        "    # Display the app's title\n",
        "    st.title(\"Talk to your PDF\")\n",
        "\n",
        "    # Create a file uploader widget for PDF files\n",
        "    uploaded_file = st.file_uploader(\"Upload your PDF\", type=[\"pdf\"])\n",
        "\n",
        "    # Check if a file has been uploaded\n",
        "    if uploaded_file is not None:\n",
        "        # Display an animation while processing the uploaded PDF\n",
        "        with st_lottie_spinner(loading_animation, quality='high', height='100px', width='100px'):\n",
        "            process_pre_run(uploaded_file)  # Preprocess the uploaded file\n",
        "\n",
        "        # Securely upload the processed file to Google Drive\n",
        "        upload_to_google_drive(uploaded_file)\n",
        "\n",
        "        # Instantiate the service class for intent processing\n",
        "        service_class = IntentService()\n",
        "\n",
        "        # Create a form for user's questions about the PDF content\n",
        "        with st.form(key='question_form'):\n",
        "            user_question = st.text_input(\"Ask a question about the PDF content:\", key=\"question_input\")\n",
        "            submit_button = st.form_submit_button(label='Ask')\n",
        "\n",
        "        # Process the question if the submit button is pressed\n",
        "        if submit_button:\n",
        "            result = process_user_question(service_class, user_question)\n",
        "\n",
        "            if result[0] is not None:  # If the question is related to the PDF content\n",
        "                vectorized_question, question = result\n",
        "\n",
        "                with st_lottie_spinner(loading_animation, quality='high', height='100px', width='100px'):\n",
        "                    retrieved_info = process_retrieval(vectorized_question)  # Retrieve relevant information\n",
        "\n",
        "                    final_response = process_response(retrieved_info, question)  # Generate and display response\n",
        "                    st.write(final_response)\n",
        "\n",
        "\n",
        "# Entry point of the Streamlit app\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}